---
title: "rf_feature_selection"
author: "Ethan Greig"
date: "1/26/2019"
output: html_document
---

```{r}
library(knitr)
source(purl('import_and_generate_series.Rmd', output = tempfile()))
# Note that only data from 2008-2017 is imported here. 2018 data is kept as a holdout set
```

# %IncMSE or MeanDecreaseAccuracy (MDA): a feature with a low MDA indicates that if its values are permuted in the training set, the model does not perform much worse. This either indicates that the feature is worthless or that the information in this feature can be gathered from other features instead. So a higher MDA feature is likely to have more unique IV compared to all other features.

# IncNodePurity or MeanDecreaseGini (MDG): a feature with a low MDA indicates that when a decision tree uses that feature to split a node, it does not decrease cross entropy with the target. So a high MDA means that a feature provides at least some predictive value at some level of some trees.

# Random Forest (Regression) Feature Importance
```{r}
library(randomForest)
dfr1 <- df
dStats_regr1 <- dStats

# Train a massive random forest so the importance metrics have low noise
#huge_model_r1 <- suppressWarnings(randomForest(`Home_Won` ~ ., data=dfr1, ntree=1000000, mtry=6, nodesize=60, importance=TRUE))
#print(huge_model_r1$importance[huge_model_r1$importance[,1]<0,])

# Remove features with negative MDA
dStats_regr2 <- dStats_regr1[!dStats_regr1%in%c('dxGA.60_55', 'dENGD', 'dGF.60_all', 'dxGA.60_all', 'dPP.', 'dROW')]
dfr2 <- dfr1[,c('Home_Won', dStats_regr2)]
```

```{r}
# Try a second time (with scaled-down hyperparams) to identify more weak features
#huge_model_r2 <- suppressWarnings(randomForest(`Home_Won` ~ ., data=dfr2, ntree=1000000, mtry=5, nodesize=60, importance=TRUE))
#print(huge_model_r2$importance)

# Remove features with MDA<5e-04 and MDG < 0.4
dStats_regr3 <- dStats_regr2[!dStats_regr2%in%c('dxGF.60_55', 'dSh._55', 'dSv._55', 'dW')]
dfr3 <- dfr2[,c('Home_Won', dStats_regr3)]
```

```{r}
# Try a third time (with scaled-down hyperparams) to identify more weak features
#huge_model_r3 <- suppressWarnings(randomForest(`Home_Won` ~ ., data=dfr3, ntree=1000000, mtry=4, nodesize=60, importance=TRUE))
#print(huge_model_r3$importance)

# Remove features with MDA<0.001 and MDG<0.5
dStats_regr4 <- dStats_regr3[!dStats_regr3%in%c('dGA.60_55', 'dTime_Led')]
dfr4 <- dfr3[,c('Home_Won', dStats_regr4)]
```

# Random Forest (Classification) Feature Importance
```{r}
library(randomForest)
dfc1 <- df
dfc1$Home_Won <- as.factor(dfc1$Home_Won)
dStats_clas1 <- dStats

# Train a massive random forest so the importance metrics have low noise
#huge_model_c1 <- suppressWarnings(randomForest(`Home_Won` ~ ., data=dfc1, ntree=1000000, mtry=6, nodesize=60, importance=TRUE))
#print(huge_model_c1$importance[huge_model_c1$importance[,3]<0,])

# Remove features with negative MDA
dStats_clas2 <- dStats_clas1[!dStats_clas1%in%c('dxGA.60_55', 'dSh._55', 'dENGD', 'dGF.60_all', 'dxGA.60_all', 'dPP.')]
dfc2 <- dfc1[,c('Home_Won', dStats_clas2)]
```

```{r}
# Train a second time (with scaled-down hyperparams)  to identify more weak features
#huge_model_c2 <- suppressWarnings(randomForest(`Home_Won` ~ ., data=dfc2, ntree=1000000, mtry=5, nodesize=60, importance=TRUE))
#print(huge_model_c2$importance)

# Remove features with MDA<5e-04 and MDG<0.5
dStats_clas3 <- dStats_clas2[!dStats_clas2%in%c('dGA.60_55', 'dxGF.60_55', 'dSv._55', 'dW', 'dROW')]
dfc3 <- dfc2[,c('Home_Won', dStats_clas3)]
```

```{r}
# Train a third time (with scaled-down hyperparams)  to identify more weak features
#huge_model_c3 <- suppressWarnings(randomForest(`Home_Won` ~ ., data=dfc3, ntree=1000000, mtry=4, nodesize=60, importance=TRUE))
#print(huge_model_c3$importance)

# Remove features with MDA<1e-03 and MDG<0.5
dStats_clas4 <- dStats_clas3[!dStats_clas3%in%c('dCA.60_55', 'dTime_Led')]
dfc4 <- dfc3[,c('Home_Won', dStats_clas4)]
```