---
title: "rf_feature_subsets"
author: "Ethan Greig"
date: "2/2/2019"
output: html_document
---

```{r, include=FALSE}
library(knitr)
source(purl('import_and_generate_series.Rmd', output = tempfile()))
# Note that only data from 2008-2017 is imported here. 2018 data is kept as a holdout set
```

# %IncMSE or MeanDecreaseAccuracy (MDA): a feature with a low MDA indicates that if its values are permuted in the training set, the model does not perform much worse. This either indicates that the feature is worthless or that the information in this feature can be gathered from other features instead. So a higher MDA feature is likely to have more unique IV compared to all other features.

# IncNodePurity or MeanDecreaseGini (MDG): a feature with a low MDA indicates that when a decision tree uses that feature to split a node, it does not decrease cross entropy with the target. So a high MDA means that a feature provides at least some predictive value at some level of some trees.

# Random Forest (Regression) Feature Importance
```{r}
library(randomForest)
df_full <- df[,c('Home_Won', dStats)]

# Train a massive random forest so the importance metrics have low noise
#imp_model_1 <- suppressWarnings(randomForest(`Home_Won` ~ ., data=df_full, ntree=1000000, mtry=1, nodesize=64, importance=TRUE))
#print(imp_model_1$importance)

# For each 55/all pair of features, remove the one with lower importance if MDA < 0.35
dStats_unpaired <- dStats[!dStats%in%c('dCF._55', 'dCF.60_all', 'dCA.60_all', 'dGF.60_all', 'dGA.60_55', 'dxGF._55', 'dxGA.60_55', 'dxGF.60_55', 'dSh._55', 'dSv._55', 'dBerg')]
df_unpaired <- df[,c('Home_Won', dStats_unpaired)]
```

```{r}
# Train a second time (with scaled-down hyperparams)  to identify more weak features
#imp_model_2 <- suppressWarnings(randomForest(`Home_Won` ~ ., data=df_unpaired, ntree=1000000, mtry=1, nodesize=64, importance=TRUE))
#print(imp_model_2$importance)

# Remove stats with MDA < 2e-03 and MDG < 0.46
dStats_10 <- dStats_unpaired[!dStats_unpaired%in%c('dCA.60_55', 'dENGD', 'dxGF.60_all', 'dxGA.60_all', 'dSh._all', 'dSv._all', 'dHDCF._55', 'dTime_Led', 'dPP.', 'dW')]
df_10 <- df_unpaired[,c('Home_Won', dStats_10)]
```

```{r}
# Train a third time (with scaled-down hyperparams)  to identify more weak features
#imp_model_3 <- suppressWarnings(randomForest(`Home_Won` ~ ., data=df_10, ntree=1000000, mtry=1, nodesize=64, importance=TRUE))
#print(imp_model_3$importance)
#print(cor(df_10))

# Generate a list features which have far higher MDA than all others
dStats_4 <- c('dCF.60_55', 'dGF._55', 'dGF._all', 'dxGF._all')

# Include dPK because it has relevanceance and is uncorrelated with all other important variables 
dStats_5 <- c("dCF.60_55", "dGF._55","dGF._all", "dxGF._all",  "dPK.")
```



