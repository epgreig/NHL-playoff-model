---
title: "rf_final_tuning.Rmd"
author: "Ethan Greig"
date: "1/27/2019"
output: html_document
---

```{r, include=FALSE}
library(knitr)
source(purl('rf_feature_selection.Rmd', output = tempfile()))
source(purl('random_forest_tuning.Rmd', output = tempfile()))
source(purl('cross_validation.Rmd', output = tempfile()))
```

## Define subsets of full features
```{r}
df_11 <- df[,c('Home_Won', dStats_11)]
df_11_class <- df_11
df_11_class$Home_Won <- as.factor(df_11_class$Home_Won)

df_5_uncorr <- df[,c('Home_Won', dStats_5_uncorr)]
df_5_uncorr_class <- df_5_uncorr
df_5_uncorr_class$Home_Won <- as.factor(df_5_uncorr_class$Home_Won)

df_4_best <- df[,c('Home_Won', dStats_4_best)]
df_4_best_class <- df_4_best
df_4_best_class$Home_Won <- as.factor(df_4_best_class$Home_Won)
```

## Hyperparameter Tuning on reduced feature set RF (11 features)
```{r}
# Grid Search using randomForest's built in OOB error (not cross-validated Log Loss).
# Prioritize lower mtry (less correlation btw trees) and higher nodesize (smoothing out regression)

# Grids attempted:
#rf_grid(df_11_class, mtrys=1:3, nodesizes=30:100, ntrees=5000)
#rf_grid(df_11_class, mtrys=1, nodesizes=65:72, ntrees=30000)

# result: mtry=1, nodesize=70
```

## Hyperparameter Tuning on reduced feature set RF (5 uncorrelated features)
```{r}
# Grid Search using randomForest's built in OOB error (not cross-validated Log Loss)
# Prioritize lower mtry (less correlation btw trees) and higher nodesize (smoothing out regression)

# Grids attempted:
#rf_grid(df_5_uncorr_class, mtrys=1:2, nodesizes=20:100, ntrees=10000)
#rf_grid(df_5_uncorr_class, mtrys=1, nodesizes=60:80, ntrees=30000)

# result: mtry=1, nodesize=70
```

## Hyperparameter Tuning on reduced feature set RF (4 best features)
```{r}
# Grid Search using randomForest's built in OOB error (not cross-validated Log Loss)
# Prioritize lower mtry (less correlation btw trees) and higher nodesize (smoothing votes)

# Grids attempted:
#rf_grid(df_4_best_class, mtrys=1:2, nodesizes=20:100, ntrees=10000)
#rf_grid(df_4_best_class, mtrys=1, nodesizes=35:50, ntrees=30000)

# result: mtry=1, nodesize=45
```


## Random Forest (Classification) Results, Tuned on subsets of features
```{r}
# FULL FEATURES
# Log Loss: 0.672 (sd 0.10)
# Accuracy: 62.2% (sd 0.11)

cv_rf_class(df_11, nfolds=10, nruns=64, mtry=2, nodesize=62, ntrees=2500)
# 13 FEATURES
# Log Loss: 0.669 (sd 0.11)
# Accuracy: 64.7% (sd 0.12)

cv_rf_class(df_5_uncorr, nfolds=10, nruns=32, mtry=1, nodesize=60, ntrees=2500)
# 7 FEATURES
# Log Loss: 0.708 (sd 0.14)
# Accuracy: 61.7% (sd 0.12)

cv_rf_class(df_4_best, nfolds=10, nruns=32, mtry=1, nodesize=40, ntrees=2500)
# 4 FEATURES
# Log Loss: 0.690 (sd 0.16)
# Accuracy: 64.6% (sd 0.12)
```


## Random Forest (Regression) Performance, Tuned on subsets of features
```{r}
# FULL FEATURES
# Log Loss: 0.652 (sd 0.08)
# Accuracy: 63% (sd 0.12)

cv_rf_reg(df_11, nfolds=10, nruns=64, mtry=2, nodesize=62, ntrees=2500)
# 13 FEATURES
# Log Loss: 0.633 (sd 0.07)
# Accuracy: 66.9% (sd 0.12)

cv_rf_reg(df_5_uncorr, nfolds=10, nruns=32, mtry=1, nodesize=60, ntrees=2500)
# 7 FEATURES
# Log Loss: 0.632 (sd 0.07)
# Accuracy: 66.0% (sd 0.12)

cv_rf_reg(df_4_best, nfolds=10, nruns=32, mtry=1, nodesize=45, ntrees=2500)
# 4 FEATURES
# Log Loss: 0.621 (sd 0.08)
# Accuracy: 69.0% (sd 0.11)
```

