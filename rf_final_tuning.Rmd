---
title: "rf_final_tuning.Rmd"
author: "Ethan Greig"
date: "1/27/2019"
output: html_document
---

```{r, include=FALSE}
library(knitr)
source(purl('random_forest_tuning.Rmd', output = tempfile()))
source(purl('cross_validation.Rmd', output = tempfile()))
source(purl('rf_feature_selection.Rmd', output = tempfile()))
```

## Define subsets of full features
```{r}
df_10 <- df[,c('Home_Won', dStats_10)]
df_best <- df[,c('Home_Won', dStats_best)]
df_test1 <- df[,c('Home_Won', dStats_test1)]
df_test2 <- df[,c('Home_Won', dStats_test2)]
```

## Hyperparameter Tuning on reduced feature set RF (10 features)
```{r}
# Grid Search using randomForest's built in OOB error (not cross-validated Log Loss).
# Prioritize lower mtry (less correlation btw trees) and higher nodesize (smoothing out regression)

# Grids attempted:
rf_grid(df_10, mtrys=1:2, nodesizes=30:100, ntrees=5000)
#rf_grid(df_10, mtrys=1, nodesizes=48:65, ntrees=30000)

# result: mtry=1, nodesize=55
```

## Hyperparameter Tuning on reduced feature set RF (4 best features)
```{r}
# Grid Search using randomForest's built in OOB error (not cross-validated Log Loss)
# Prioritize lower mtry (less correlation btw trees) and higher nodesize (smoothing out regression)

# Grids attempted:
#rf_grid(df_best, mtrys=1:2, nodesizes=20:100, ntrees=5000)
rf_grid(df_best, mtrys=1, nodesizes=40:80, ntrees=10000)

# result: mtry=1, nodesize=73 or 65 or 40
```

## Hyperparameter Tuning on reduced feature set RF (4 best features)
```{r}
# Grid Search using randomForest's built in OOB error (not cross-validated Log Loss)
# Prioritize lower mtry (less correlation btw trees) and higher nodesize (smoothing votes)

# Grids attempted:
#rf_grid(df_4_best_class, mtrys=1:2, nodesizes=20:100, ntrees=10000)
#rf_grid(df_4_best_class, mtrys=1, nodesizes=35:50, ntrees=30000)

# result: mtry=1, nodesize=45
```


## Random Forest (Classification) Results, Tuned on subsets of features
```{r}
# FULL FEATURES
# Log Loss: 0.672 (sd 0.10)
# Accuracy: 62.2% (sd 0.11)

cv_rf_class(df_11, nfolds=10, nruns=64, mtry=2, nodesize=62, ntrees=2500)
# 13 FEATURES
# Log Loss: 0.669 (sd 0.11)
# Accuracy: 64.7% (sd 0.12)

cv_rf_class(df_5_uncorr, nfolds=10, nruns=32, mtry=1, nodesize=60, ntrees=2500)
# 7 FEATURES
# Log Loss: 0.708 (sd 0.14)
# Accuracy: 61.7% (sd 0.12)

cv_rf_class(df_4_best, nfolds=10, nruns=32, mtry=1, nodesize=40, ntrees=2500)
# 4 FEATURES
# Log Loss: 0.690 (sd 0.16)
# Accuracy: 64.6% (sd 0.12)
```


## Random Forest (Regression) Performance, Tuned on subsets of features
```{r}
# FULL FEATURES
# Log Loss: 0.652 (sd 0.08)
# Accuracy: 63% (sd 0.12)

cv_rf_reg(df_11, nfolds=10, nruns=64, mtry=2, nodesize=62, ntrees=2500)
# 13 FEATURES
# Log Loss: 0.633 (sd 0.07)
# Accuracy: 66.9% (sd 0.12)

cv_rf_reg(df_5_uncorr, nfolds=10, nruns=32, mtry=1, nodesize=60, ntrees=2500)
# 7 FEATURES
# Log Loss: 0.632 (sd 0.07)
# Accuracy: 66.0% (sd 0.12)

cv_rf_reg(df_4_best, nfolds=10, nruns=32, mtry=1, nodesize=45, ntrees=2500)
# 4 FEATURES
# Log Loss: 0.621 (sd 0.08)
# Accuracy: 69.0% (sd 0.11)
```

